{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fdb7f4-dbe7-4d6a-94ff-8560b5228704",
   "metadata": {},
   "source": [
    "# <span style=\"color:green\">Oregon Hydrographic Area Historical ET & Consumptive Use Geodatabase: OWRD, DRI, and OpenET</span>\n",
    "\n",
    "<center><img src=\"data_viz_et.png\" width=\"900\"/></center>\n",
    "<!-- ![Data Viz](data_viz_et.png \"ET/CU Geodatabase\") -->\n",
    "\n",
    "______________________\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0396b8-9a86-464e-b818-e8ecf72cc691",
   "metadata": {},
   "source": [
    "\n",
    "# ET/CU field-level data visualization & retrieval [tool](https://dri-apps.projects.earthengine.app/view/owrd-oregon-etcu-field-summaries)\n",
    "> Retrieve monthly/annual timeseries data for individual field boundaries for 1985-2024\n",
    "\n",
    "<center><img src=\"data_viz_field_level_tool.png\" width=\"1200\"/></center>\n",
    "\n",
    "<br>\n",
    "\n",
    "# Python-based ET/CU data aggregation notebook\n",
    "> Python workflow to summarize 40 years (1985-2024) of monthly/annual data variables for watershed or irrigation district boundaries\n",
    "\n",
    "## Data Variable Definitions:\n",
    "* OpenET ensemble actual ET ($ET_{a}$)\n",
    "* Bias-corrected gridMET reference ET ($ET_{o}$)\n",
    "* Effective precipitation ($P_{rz}$)\n",
    "* Total precipitation (PPT)\n",
    "* Crop-specific potential ET ($ET_{c}$)\n",
    "* Consumptive use (CU)\n",
    "* Net irrigation water requirement (NIWR)\n",
    "* Applied water (AW)\n",
    "\n",
    "\n",
    "## <span style=\"color:red\">Please run this notebook using a local Python environment. NOTE: this notebook will download the entire Oregon geodatabase (11 GB zipped, 30GB unzipped) to the local system.</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610cb91a-f78b-4289-8ab8-44a70551c4dd",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "# Overview of Walkthrough:\n",
    "\n",
    "INTRODUCTION_________________________________\n",
    "\n",
    "1. Import Python packages\n",
    "    - [geopandas](https://geopandas.org/en/stable/#) is a python-based geospatial analysis tool.\n",
    "    - [pandas](https://pandas.pydata.org/) is a python-based data analysis and manipulation tool.\n",
    "    - [polars](https://pola.rs/) is an efficient/fast python-based data analysis and manipulation tool.\n",
    "    - [requests](https://pypi.org/project/requests/) is a simple, yet elegant, HTTP library.\n",
    "    - [py7zr](https://pypi.org/project/py7zr/) supports 7zip archive compression, decompression, encryption and decryption.\n",
    "    - [folium](https://python-visualization.github.io/folium/latest/index.html) is a python-based interactive geospatial analysis and visualization tool. Avoids the use of Earth Engine and allows us to visualize locally-stored geospatial data and the geodatabase.\n",
    "2. Download & Visualize the Historical ET/CU Geodatabase\n",
    "\n",
    "TIMESERIES EXPORT_____________________________\n",
    "\n",
    "3. Export spatially and temporally aggregated ET/CU data\n",
    "\n",
    "DATA COMPARISON______________________________\n",
    "\n",
    "4. Compare applied water estimates to total diversions for Tumalo Irrigation District\n",
    "> monthly & annual plotting, canal diversion comparisons, and conveyance loss estimates\n",
    "\n",
    "___\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccd537e-f918-4093-953c-2469f736fef5",
   "metadata": {},
   "source": [
    "## 1. Import Python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9c80a0c0-dd63-46fc-878d-8c7c4b759279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import folium\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import py7zr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8e1dc9-490c-4d61-97b5-dcc07e76ca06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the path to the main directory\n",
    "# root_directory = r'/Users/blakeminor/Documents/GitHub/OR_OWRD_OSU_training' # Mac\n",
    "root_directory = r'C:\\Users\\bminor\\Documents\\GitHub\\OR_OWRD_OSU_training' # Windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde99ef0-74be-4249-b29e-5515233137b7",
   "metadata": {},
   "source": [
    "## 2. Download & Visualize the Historical ET/CU Geodatabase\n",
    "> Download the geodatabase programmatically using **requests** below<br><br>\n",
    "> If you prefer a manual download, you can login with the username and passwor [here](http://crushftp.dri.edu/)<br>\n",
    "> Once downloaded, move the 7-zip file into the \"data\" subfolder and then unzip the file<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d5e76aed-22bb-474d-aa37-a08713409fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# username and password required for the DRI CrushFTP URL\n",
    "usernm = ''\n",
    "passwrd = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4215e0-60ff-491c-8288-2b38ac062324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_zip_https(url, username, password, local_filename):\n",
    "    try:\n",
    "        # Use HTTP Basic Authentication\n",
    "        r = requests.get(url, auth=HTTPBasicAuth(username, password), stream=True)\n",
    "        r.raise_for_status() # Raise an exception for bad status codes\n",
    "\n",
    "        with open(local_filename, 'wb') as f:\n",
    "            for block in r.iter_content(1024):\n",
    "                if not block:\n",
    "                    break\n",
    "                f.write(block)\n",
    "        print(f\"File '{local_filename}' downloaded successfully via HTTPS.\")\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading file: {e}\")\n",
    "\n",
    "# geodatabase URL\n",
    "server_url = \"https://crushftp.dri.edu/preliminary_or_field_geopackage.7z\"\n",
    "\n",
    "# local path for storing the geodatabase\n",
    "local_file_path = os.path.join(root_directory, 'data', 'preliminary_or_field_geopackage.7z')\n",
    "\n",
    "if not os.path.isfile(local_file_path):\n",
    "    print('Atemmpting to download the geodatabase from CrushFTP, please wait ')\n",
    "    download_zip_https(server_url, usernm, passwrd, local_file_path)\n",
    "else:\n",
    "    print('Geodatabase has already been downloaded, skip to the next cell')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbdb9304-fb8e-47b0-93bc-3c00e8e9bc9b",
   "metadata": {},
   "source": [
    "### Unzip the geodatabase if needed\n",
    "> Unzip the geodatabase programmatically using **py7zr** below<br><br>\n",
    "> To manually unzip the geodatabase, please use the [7-zip](https://www.7-zip.org/) application for Windows machines or the built-in Archive Utility application on MacOS/Linux machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ff038c-38a9-4760-be70-3680d1c1e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# local path to the geodatabase\n",
    "local_file_path = os.path.join(root_directory, 'data', 'preliminary_or_field_geopackage.7z')\n",
    "\n",
    "if os.path.isfile(local_file_path):\n",
    "    if not os.path.isfile(os.path.join(root_directory, 'data', 'preliminary_or_field_geopackage.gpkg')):\n",
    "        try:\n",
    "            with py7zr.SevenZipFile(local_file_path, 'r') as archive:\n",
    "                archive.extractall(path=os.path.join(root_directory, 'data', 'preliminary_or_field_geopackage.gpkg'))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    else:\n",
    "        print('geodatabase has already been unzipped, skip to the next cell')\n",
    "else:\n",
    "    print('geodatabase zip file was not located, please download it using the previous code blocks or manually download it')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0047a7-f9d4-44ae-aa10-b73695a67849",
   "metadata": {},
   "source": [
    "### Read the geodatabase\n",
    "> We just want the geometries with ID's for determining which fields to include in spatial & temporal aggregations<br>\n",
    "> Only need one layer of the geodatabase for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2269fe-7adb-4307-81ad-313e9af08e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into memory\n",
    "try:\n",
    "    gdf = gpd.read_file(os.path.join(root_directory, 'data', 'preliminary_or_field_geopackage.gpkg'), \n",
    "                        layer='Oregon_Hyd_Area_Ag_Boundaries_20241016', columns=['OPENET_ID'])\n",
    "    print(gdf)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5fd68-237a-4133-9030-2297bb89eaa1",
   "metadata": {},
   "source": [
    "### Read a shapefile to define the region used for aggregating the field-level data from the geodatabase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7ff5cea4-5026-48c4-8ec0-3530d76b0524",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, define the shapefile name (example uses the Tumalo Irrigation District boundary)\n",
    "region_shapefile_name = 'tumalo_irrigation_district'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2722f33a-27e9-4064-90bc-8a4752fa64c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the file into memory\n",
    "try:\n",
    "    region_gdf = gpd.read_file(os.path.join(root_directory, 'data', f'{region_shapefile_name}.shp'))\n",
    "    print(region_gdf)\n",
    "except Exception as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ced4f1-643e-4da8-a4e9-38dd4a0c886e",
   "metadata": {},
   "source": [
    "### Use the region to process get the list of field ID's to process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31efb7d-4eab-418a-8e5a-23f0697433f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_by_centroid_within_region(gdf_to_filter, gdf_regions):\n",
    "    \"\"\"\n",
    "    Subsets a GeoDataFrame (gdf_to_filter) to keep only the geometries whose \n",
    "    centroids fall within the polygons of another GeoDataFrame (gdf_regions).\n",
    "\n",
    "    Args:\n",
    "        gdf_to_filter (gpd.GeoDataFrame): The GeoDataFrame to filter (e.g., polygons).\n",
    "        gdf_regions (gpd.GeoDataFrame): The GeoDataFrame with region polygons.\n",
    "\n",
    "    Returns:\n",
    "        gpd.GeoDataFrame: The filtered GeoDataFrame.\n",
    "    \"\"\"\n",
    "    # 1. Ensure both GeoDataFrames have the same CRS\n",
    "    if gdf_to_filter.crs != gdf_regions.crs:\n",
    "        print(f\"Reprojecting the region CRS to match the geodatabase CRS (EPSG:2994, Oregon Lambert Conformal Conic): {gdf_to_filter.crs}\")\n",
    "        gdf_regions = gdf_regions.to_crs(gdf_to_filter.crs)\n",
    "\n",
    "    # 2. Create a temporary GeoSeries of the centroids\n",
    "    # Note: .centroid can fall outside the polygon for complex shapes, \n",
    "    # use .representative_point() if you need a point guaranteed to be inside.\n",
    "    centroids = gdf_to_filter.centroid\n",
    "    \n",
    "    # Create a temporary GeoDataFrame for the join, setting centroids as the active geometry\n",
    "    gdf_centroids = gdf_to_filter.copy()\n",
    "    gdf_centroids['centroid_geometry'] = centroids\n",
    "    gdf_centroids = gdf_centroids.set_geometry('centroid_geometry')\n",
    "    \n",
    "    # 3. Perform a spatial join with predicate='within'\n",
    "    # We use an inner join to keep only records where a centroid is within a region\n",
    "    # The 'left_index=True, right_index=True' is generally not needed here, sjoin handles it.\n",
    "    join_result = gpd.sjoin(gdf_centroids, gdf_regions, how=\"inner\", predicate=\"within\")\n",
    "    \n",
    "    # The join result contains rows where centroids were within a region. \n",
    "    # The original geometry column ('geometry') is still present in the join result\n",
    "    # We can now set the active geometry back to the original polygons\n",
    "    filtered_gdf = join_result.set_geometry('geometry')\n",
    "    \n",
    "    # Drop the temporary centroid column and any extra columns from the join\n",
    "    # (e.g., 'index_right' which is added by sjoin)\n",
    "    filtered_gdf = filtered_gdf.drop(columns=['centroid_geometry', 'index_right'], errors='ignore')\n",
    "    \n",
    "    return filtered_gdf\n",
    "\n",
    "# Example Usage:\n",
    "# Assuming you have gdf_polygons and gdf_regions loaded\n",
    "# filtered_data = subset_by_centroid_within_region(gdf_polygons, gdf_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b22454-e5a8-4606-a7c8-9e2c40b6c488",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
